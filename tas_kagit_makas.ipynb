{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba3acbe",
   "metadata": {},
   "source": [
    "## Basit TaÅŸ KaÄŸÄ±t Makas Oyunu\n",
    "### Bu kodda bilgisayar tamamen rastgele oynuyor. BirkaÃ§ el oynayÄ±p sonuÃ§larÄ± gÃ¶zlemleyelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf45f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# OlasÄ± hamleler\n",
    "moves = ['taÅŸ', 'kaÄŸÄ±t', 'makas']\n",
    "\n",
    "# Skorlar\n",
    "user_score = 0\n",
    "computer_score = 0\n",
    "\n",
    "# Kazanma kurallarÄ±\n",
    "def determine_winner(user_move, comp_move):\n",
    "    if user_move == comp_move:\n",
    "        return 'beraberlik'\n",
    "    elif (user_move == 'taÅŸ' and comp_move == 'makas') or \\\n",
    "         (user_move == 'kaÄŸÄ±t' and comp_move == 'taÅŸ') or \\\n",
    "         (user_move == 'makas' and comp_move == 'kaÄŸÄ±t'):\n",
    "        return 'kullanÄ±cÄ±'\n",
    "    else:\n",
    "        return 'bilgisayar'\n",
    "\n",
    "# Oyun dÃ¶ngÃ¼sÃ¼\n",
    "for round in range(1, 10):\n",
    "    print(f\"\\n--- {round}. Tur ---\")\n",
    "    user_move = input(\"Hamleni gir (taÅŸ/kaÄŸÄ±t/makas): \").lower()\n",
    "    \n",
    "    if user_move not in moves:\n",
    "        print(\"GeÃ§ersiz hamle, tekrar deneyin.\")\n",
    "        continue\n",
    "    \n",
    "    comp_move = random.choice(moves)\n",
    "    print(f\"BilgisayarÄ±n hamlesi: {comp_move}\")\n",
    "    \n",
    "    result = determine_winner(user_move, comp_move)\n",
    "    \n",
    "    if result == 'kullanÄ±cÄ±':\n",
    "        print(\"Bu turu sen kazandÄ±n!\")\n",
    "        user_score += 1\n",
    "    elif result == 'bilgisayar':\n",
    "        print(\"Bu turu bilgisayar kazandÄ±!\")\n",
    "        computer_score += 1\n",
    "    else:\n",
    "        print(\"Beraberlik!\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Son Skor - KullanÄ±cÄ±: {user_score} | Bilgisayar: {computer_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0c2f2",
   "metadata": {},
   "source": [
    "## Markov Modeli ile TaÅŸ-KÃ¢ÄŸÄ±t-Makas\n",
    "### KullanÄ±cÄ±nÄ±n her hamlesinden sonra, bir sonraki hamlesi kaydedilir.\n",
    "\n",
    "### GeÃ§miÅŸ hamle istatistiklerine gÃ¶re, kullanÄ±cÄ± bir hamleden sonra en Ã§ok ne yapmÄ±ÅŸsa, onu yapacaÄŸÄ± varsayÄ±lÄ±r.\n",
    "\n",
    "### Bilgisayar da bu tahmine gÃ¶re ona karÅŸÄ± gelen hamleyi seÃ§er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786312c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "moves = ['taÅŸ', 'kaÄŸÄ±t', 'makas']\n",
    "beats = {'taÅŸ': 'kaÄŸÄ±t', 'kaÄŸÄ±t': 'makas', 'makas': 'taÅŸ'}\n",
    "\n",
    "# 2. derece Markov: (prev2, prev1) -> sonraki hamle sayÄ±larÄ±\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "user_history = []\n",
    "user_score = 0\n",
    "computer_score = 0\n",
    "\n",
    "def determine_winner(user_move, comp_move):\n",
    "    if user_move == comp_move:\n",
    "        return 'beraberlik'\n",
    "    elif (user_move == 'taÅŸ' and comp_move == 'makas') or \\\n",
    "         (user_move == 'kaÄŸÄ±t' and comp_move == 'taÅŸ') or \\\n",
    "         (user_move == 'makas' and comp_move == 'kaÄŸÄ±t'):\n",
    "        return 'kullanÄ±cÄ±'\n",
    "    else:\n",
    "        return 'bilgisayar'\n",
    "\n",
    "def predict_next_move(prev2, prev1):\n",
    "    key = (prev2, prev1)\n",
    "    if key not in transition_counts:\n",
    "        return random.choice(moves)\n",
    "    next_moves = transition_counts[key]\n",
    "    if not next_moves:\n",
    "        return random.choice(moves)\n",
    "    # En olasÄ± hareketi tahmin et\n",
    "    predicted = max(next_moves, key=next_moves.get)\n",
    "    return predicted\n",
    "\n",
    "for round in range(1, 11):\n",
    "    print(f\"\\n--- {round}. Tur ---\")\n",
    "    user_move = input(\"Hamleni gir (taÅŸ/kaÄŸÄ±t/makas): \").lower()\n",
    "\n",
    "    if user_move not in moves:\n",
    "        print(\"GeÃ§ersiz hamle.\")\n",
    "        continue\n",
    "\n",
    "    # Tahmin ve seÃ§im\n",
    "    if len(user_history) < 2:\n",
    "        comp_move = random.choice(moves)\n",
    "    else:\n",
    "        predicted_user_move = predict_next_move(user_history[-2], user_history[-1])\n",
    "        comp_move = beats[predicted_user_move]\n",
    "    \n",
    "    print(f\"Senin hamlen: {user_move} | BilgisayarÄ±n hamlesi: {comp_move}\")\n",
    "    \n",
    "    result = determine_winner(user_move, comp_move)\n",
    "    if result == 'kullanÄ±cÄ±':\n",
    "        print(\"Bu turu sen kazandÄ±n!\")\n",
    "        user_score += 1\n",
    "    elif result == 'bilgisayar':\n",
    "        print(\"Bu turu bilgisayar kazandÄ±!\")\n",
    "        computer_score += 1\n",
    "    else:\n",
    "        print(\"Beraberlik!\")\n",
    "\n",
    "    # Markov modelini gÃ¼ncelle\n",
    "    if len(user_history) >= 2:\n",
    "        key = (user_history[-2], user_history[-1])\n",
    "        transition_counts[key][user_move] += 1\n",
    "    user_history.append(user_move)\n",
    "\n",
    "print(f\"\\nğŸ“Š Son Skor - KullanÄ±cÄ±: {user_score} | Bilgisayar: {computer_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4bf64",
   "metadata": {},
   "source": [
    "## Q-learning ile TaÅŸ-KaÄŸÄ±t-Makas Oyunu (Temel)\n",
    "### Durum (state): KullanÄ±cÄ±nÄ±n son hamlesi (Ã¶rneÄŸin: taÅŸ, kaÄŸÄ±t, makas)\n",
    "\n",
    "### Aksiyon (action): BilgisayarÄ±n oynayacaÄŸÄ± hamle (taÅŸ, kaÄŸÄ±t, makas)\n",
    "\n",
    "### Ã–dÃ¼l (reward): Kazanma = +1, Beraberlik = 0, Kaybetme = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e621fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "moves = ['taÅŸ', 'kaÄŸÄ±t', 'makas']\n",
    "beats = {'taÅŸ': 'makas', 'kaÄŸÄ±t': 'taÅŸ', 'makas': 'kaÄŸÄ±t'}\n",
    "\n",
    "# Q-Tablosu: state (son kullanÄ±cÄ± hamlesi) -> action (bilgisayar hamlesi) deÄŸerleri\n",
    "Q = {move: {a: 0 for a in moves} for move in moves}\n",
    "\n",
    "alpha = 0.1    # Ã¶ÄŸrenme hÄ±zÄ±\n",
    "gamma = 0.9    # gelecekteki Ã¶dÃ¼llerin indirgeme faktÃ¶rÃ¼\n",
    "epsilon = 0.2  # keÅŸif oranÄ±\n",
    "\n",
    "def determine_winner(user_move, comp_move):\n",
    "    if user_move == comp_move:\n",
    "        return 0  # beraberlik\n",
    "    elif beats[comp_move] == user_move:\n",
    "        return 1  # bilgisayar kazandÄ±\n",
    "    else:\n",
    "        return -1 # kullanÄ±cÄ± kazandÄ±\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(moves)  # keÅŸif\n",
    "    else:\n",
    "        # En yÃ¼ksek Q deÄŸerine sahip hareketi seÃ§\n",
    "        max_q = max(Q[state].values())\n",
    "        actions_with_max_q = [a for a, q in Q[state].items() if q == max_q]\n",
    "        return random.choice(actions_with_max_q)\n",
    "\n",
    "user_history = []\n",
    "user_score = 0\n",
    "computer_score = 0\n",
    "\n",
    "print(\"TaÅŸ-KaÄŸÄ±t-Makas Q-learning oyunu baÅŸlÄ±yor!\")\n",
    "\n",
    "for round in range(1, 21):\n",
    "    print(f\"\\n--- {round}. Tur ---\")\n",
    "    user_move = input(\"Hamleni gir (taÅŸ/kaÄŸÄ±t/makas): \").lower()\n",
    "    if user_move not in moves:\n",
    "        print(\"GeÃ§ersiz hamle.\")\n",
    "        continue\n",
    "    \n",
    "    # Son kullanÄ±cÄ± hamlesine gÃ¶re durum belirle\n",
    "    if not user_history:\n",
    "        state = random.choice(moves)  # BaÅŸlangÄ±Ã§ iÃ§in rastgele durum\n",
    "    else:\n",
    "        state = user_history[-1]\n",
    "    \n",
    "    comp_move = choose_action(state)\n",
    "    print(f\"Senin hamlen: {user_move} | BilgisayarÄ±n hamlesi: {comp_move}\")\n",
    "    \n",
    "    reward = determine_winner(user_move, comp_move)\n",
    "    if reward == 1:\n",
    "        print(\"Bilgisayar kazandÄ±!\")\n",
    "        computer_score += 1\n",
    "    elif reward == -1:\n",
    "        print(\"Sen kazandÄ±n!\")\n",
    "        user_score += 1\n",
    "    else:\n",
    "        print(\"Beraberlik!\")\n",
    "\n",
    "    # Q-Tablosunu gÃ¼ncelle\n",
    "    if user_history:\n",
    "        old_state = user_history[-1]\n",
    "        old_action = comp_move\n",
    "        # Bir sonraki durum\n",
    "        next_state = user_move\n",
    "        max_future_q = max(Q[next_state].values())\n",
    "        Q[old_state][old_action] = Q[old_state][old_action] + alpha * (reward + gamma * max_future_q - Q[old_state][old_action])\n",
    "\n",
    "    user_history.append(user_move)\n",
    "\n",
    "print(f\"\\nğŸ“Š Son Skor - KullanÄ±cÄ±: {user_score} | Bilgisayar: {computer_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896cc19",
   "metadata": {},
   "source": [
    "## Ä°ki AI AjanÄ±yla Q-learning TaÅŸ-KaÄŸÄ±t-Makas Oyunu\n",
    "### Durum (state): Rakibin Ã¶nceki hamlesi (taÅŸ, kaÄŸÄ±t veya makas)\n",
    "### Ä°lk hamle iÃ§in Ã¶zel durum: \"BaÅŸlangÄ±Ã§\" (rakibin hamlesi yok)\n",
    "### Aksiyon (action): Oyuncunun oynayabileceÄŸi hamleler: taÅŸ, kaÄŸÄ±t, makas\n",
    "### Ã–dÃ¼l (reward): Kazanma = +1 Beraberlik = 0 Kaybetme = -1\n",
    "\n",
    "### Her bir AI, rakibin son hamlesini gÃ¶zlemleyerek hangi hamleyi yaparsa kazanma ÅŸansÄ±nÄ±n yÃ¼ksek olduÄŸunu Q-learning ile Ã¶ÄŸrenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee9f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tur 100 ---\n",
      "AI_1 Skor: 24, AI_2 Skor: 30, Beraberlik: 46\n",
      "--- Tur 200 ---\n",
      "AI_1 Skor: 67, AI_2 Skor: 60, Beraberlik: 73\n",
      "--- Tur 300 ---\n",
      "AI_1 Skor: 94, AI_2 Skor: 82, Beraberlik: 124\n",
      "--- Tur 400 ---\n",
      "AI_1 Skor: 135, AI_2 Skor: 126, Beraberlik: 139\n",
      "--- Tur 500 ---\n",
      "AI_1 Skor: 167, AI_2 Skor: 172, Beraberlik: 161\n",
      "\n",
      "ğŸ“Š Oyun Sonucu:\n",
      "AI_1 kazandÄ±ÄŸÄ± tur sayÄ±sÄ±: 167\n",
      "AI_2 kazandÄ±ÄŸÄ± tur sayÄ±sÄ±: 172\n",
      "Beraberlik sayÄ±sÄ±: 161\n",
      "Kazanan: AI_2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "moves = ['taÅŸ', 'kaÄŸÄ±t', 'makas']\n",
    "beats = {'taÅŸ': 'makas', 'kaÄŸÄ±t': 'taÅŸ', 'makas': 'kaÄŸÄ±t'}\n",
    "\n",
    "# Q-learning parametreleri\n",
    "alpha = 0.1    # Ã¶ÄŸrenme hÄ±zÄ±\n",
    "gamma = 0.9    # indirim faktÃ¶rÃ¼\n",
    "epsilon = 0.1  # keÅŸif oranÄ±\n",
    "episodes = 500\n",
    "\n",
    "# Q tablolarÄ± (state=son hamle, action=hamle)\n",
    "Q1 = {move: np.zeros(len(moves)) for move in moves + ['BaÅŸlangÄ±Ã§']}\n",
    "Q2 = {move: np.zeros(len(moves)) for move in moves + ['BaÅŸlangÄ±Ã§']}\n",
    "\n",
    "def choose_action(Q, state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(range(len(moves)))\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def reward(p1_move, p2_move):\n",
    "    if p1_move == p2_move:\n",
    "        return 0\n",
    "    elif beats[p1_move] == p2_move:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Oyun sonucu skorlarÄ±\n",
    "ai1_score = 0\n",
    "ai2_score = 0\n",
    "draw_count = 0\n",
    "\n",
    "state1 = 'BaÅŸlangÄ±Ã§'\n",
    "state2 = 'BaÅŸlangÄ±Ã§'\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    action1 = choose_action(Q1, state1)\n",
    "    action2 = choose_action(Q2, state2)\n",
    "\n",
    "    move1 = moves[action1]\n",
    "    move2 = moves[action2]\n",
    "\n",
    "    r1 = reward(move1, move2)\n",
    "    r2 = reward(move2, move1)  # tersi\n",
    "\n",
    "    # Q-table gÃ¼ncelleme\n",
    "    next_state1 = move2\n",
    "    next_state2 = move1\n",
    "\n",
    "    Q1[state1][action1] = Q1[state1][action1] + alpha * (r1 + gamma * max(Q1[next_state1]) - Q1[state1][action1])\n",
    "    Q2[state2][action2] = Q2[state2][action2] + alpha * (r2 + gamma * max(Q2[next_state2]) - Q2[state2][action2])\n",
    "\n",
    "    state1 = next_state1\n",
    "    state2 = next_state2\n",
    "\n",
    "    # Skor gÃ¼ncelleme\n",
    "    if r1 == 1:\n",
    "        ai1_score += 1\n",
    "    elif r1 == -1:\n",
    "        ai2_score += 1\n",
    "    else:\n",
    "        draw_count += 1\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"--- Tur {episode} ---\")\n",
    "        print(f\"AI_1 Skor: {ai1_score}, AI_2 Skor: {ai2_score}, Beraberlik: {draw_count}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Oyun Sonucu:\")\n",
    "print(f\"AI_1 kazandÄ±ÄŸÄ± tur sayÄ±sÄ±: {ai1_score}\")\n",
    "print(f\"AI_2 kazandÄ±ÄŸÄ± tur sayÄ±sÄ±: {ai2_score}\")\n",
    "print(f\"Beraberlik sayÄ±sÄ±: {draw_count}\")\n",
    "\n",
    "if ai1_score > ai2_score:\n",
    "    print(\"Kazanan: AI_1\")\n",
    "elif ai2_score > ai1_score:\n",
    "    print(\"Kazanan: AI_2\")\n",
    "else:\n",
    "    print(\"Oyun berabere bitti!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2cea56",
   "metadata": {},
   "source": [
    "## Nash Q-learning ile Ä°ki AI AjanÄ±nÄ±n TaÅŸ-KaÄŸÄ±t-Makas Oyunu  \n",
    "### Durum (state): Rakibin Ã¶nceki hamlesi (taÅŸ, kaÄŸÄ±t veya makas) veya \"BaÅŸlangÄ±Ã§\"  \n",
    "### Aksiyon (action): Oyuncunun oynayabileceÄŸi hamleler: taÅŸ, kaÄŸÄ±t, makas  \n",
    "### Ã–dÃ¼l (reward): Kazanma = +1, Beraberlik = 0, Kaybetme = -1  \n",
    "\n",
    "### Ã–zet:  \n",
    "- Her turda iki AI ajanÄ±, mevcut durumdaki Q-matrislerine gÃ¶re oyun matrisini oluÅŸturur.  \n",
    "- Nashpy kÃ¼tÃ¼phanesi kullanÄ±larak bu matrisler iÃ§in Nash dengesi (karÅŸÄ±lÄ±klÄ± en iyi strateji karÄ±ÅŸÄ±mÄ±) hesaplanÄ±r.  \n",
    "- Ajanlar, Nash dengesinden elde edilen karma stratejilere gÃ¶re hareket seÃ§er.  \n",
    "- Q tablolarÄ±, Ã¶dÃ¼l ve gelecekteki Nash dengesine gÃ¶re beklenen getirilerle gÃ¼ncellenir.  \n",
    "- BÃ¶ylece ajanlar sadece kendi Ã¶dÃ¼llerini deÄŸil, karÅŸÄ± tarafÄ±n stratejik davranÄ±ÅŸlarÄ±nÄ± da hesaba katarak Ã¶ÄŸrenir.  \n",
    "- Bu yÃ¶ntem, klasik Q-learningâ€™den farklÄ± olarak Ã§ok oyunculu oyunlarda denge stratejilerini Ã¶ÄŸrenmeyi hedefler.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8022fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nashpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5d0fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tur 200 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 200\n",
      "\n",
      "--- Tur 400 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 400\n",
      "\n",
      "--- Tur 600 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 600\n",
      "\n",
      "--- Tur 800 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 800\n",
      "\n",
      "--- Tur 1000 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1000\n",
      "\n",
      "--- Tur 1200 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1200\n",
      "\n",
      "--- Tur 1400 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1400\n",
      "\n",
      "--- Tur 1600 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1600\n",
      "\n",
      "--- Tur 1800 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1800\n",
      "\n",
      "--- Tur 2000 ---\n",
      "AI_1 hamlesi: taÅŸ | AI_2 hamlesi: taÅŸ\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 2000\n",
      "\n",
      "=== Oyun Sonu ===\n",
      "AI_1 kazandÄ±: 0\n",
      "AI_2 kazandÄ±: 0\n",
      "Beraberlikler: 2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nashpy as nash\n",
    "\n",
    "# TaÅŸ-kaÄŸÄ±t-makas tanÄ±mÄ±\n",
    "moves = ['taÅŸ', 'kaÄŸÄ±t', 'makas']\n",
    "num_moves = len(moves)\n",
    "\n",
    "# Parametreler\n",
    "alpha = 0.1       # Ã¶ÄŸrenme hÄ±zÄ±\n",
    "gamma = 0.9       # indirim faktÃ¶rÃ¼\n",
    "episodes = 2000   # toplam oyun sayÄ±sÄ±\n",
    "\n",
    "# Q tablolarÄ±: durum 'BaÅŸlangÄ±Ã§' iÃ§in 3x3 matrisler\n",
    "Q1 = {'BaÅŸlangÄ±Ã§': np.zeros((num_moves, num_moves))}\n",
    "Q2 = {'BaÅŸlangÄ±Ã§': np.zeros((num_moves, num_moves))}\n",
    "\n",
    "def reward(p1_move, p2_move):\n",
    "    if p1_move == p2_move:\n",
    "        return 0, 0\n",
    "    elif (p1_move == 'taÅŸ' and p2_move == 'makas') or \\\n",
    "         (p1_move == 'kaÄŸÄ±t' and p2_move == 'taÅŸ') or \\\n",
    "         (p1_move == 'makas' and p2_move == 'kaÄŸÄ±t'):\n",
    "        return 1, -1\n",
    "    else:\n",
    "        return -1, 1\n",
    "\n",
    "def get_nash_strategy(Q1_mat, Q2_mat):\n",
    "    \"\"\"\n",
    "    Verilen iki Q matrisi iÃ§in Nash dengesini hesaplar.\n",
    "    \"\"\"\n",
    "    game = nash.Game(Q1_mat, Q2_mat)\n",
    "    equilibria = list(game.support_enumeration())\n",
    "    if equilibria:\n",
    "        sigma1, sigma2 = equilibria[0]\n",
    "    else:\n",
    "        # Nash bulunamazsa eÅŸit olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±\n",
    "        sigma1 = np.ones(num_moves) / num_moves\n",
    "        sigma2 = np.ones(num_moves) / num_moves\n",
    "    return sigma1, sigma2\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ durumu\n",
    "state = 'BaÅŸlangÄ±Ã§'\n",
    "\n",
    "# Kazanma sayacÄ±\n",
    "score_1 = 0\n",
    "score_2 = 0\n",
    "draws = 0\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    # Durum iÃ§in Q tablosu yoksa oluÅŸtur\n",
    "    if state not in Q1:\n",
    "        Q1[state] = np.zeros((num_moves, num_moves))\n",
    "        Q2[state] = np.zeros((num_moves, num_moves))\n",
    "\n",
    "    Q1_state = Q1[state]\n",
    "    Q2_state = Q2[state]\n",
    "\n",
    "    # Nash stratejilerini bul\n",
    "    sigma1, sigma2 = get_nash_strategy(Q1_state, Q2_state)\n",
    "\n",
    "    # Stratejilere gÃ¶re hamle seÃ§\n",
    "    action1 = np.random.choice(num_moves, p=sigma1)\n",
    "    action2 = np.random.choice(num_moves, p=sigma2)\n",
    "\n",
    "    move1 = moves[action1]\n",
    "    move2 = moves[action2]\n",
    "\n",
    "    # Ã–dÃ¼ller\n",
    "    r1, r2 = reward(move1, move2)\n",
    "\n",
    "    # Gelecek durum (Ã¶rnek basit: rakibin son hamlesi)\n",
    "    next_state = move2\n",
    "    if next_state not in Q1:\n",
    "        Q1[next_state] = np.zeros((num_moves, num_moves))\n",
    "        Q2[next_state] = np.zeros((num_moves, num_moves))\n",
    "\n",
    "    # Gelecek durumun Nash stratejileri\n",
    "    sigma1_next, sigma2_next = get_nash_strategy(Q1[next_state], Q2[next_state])\n",
    "\n",
    "    # Gelecek durumun beklenen Ã¶dÃ¼lleri\n",
    "    expected_q1 = 0\n",
    "    expected_q2 = 0\n",
    "    for a1 in range(num_moves):\n",
    "        for a2 in range(num_moves):\n",
    "            expected_q1 += sigma1_next[a1] * Q1[next_state][a1, a2] * sigma2_next[a2]\n",
    "            expected_q2 += sigma1_next[a1] * Q2[next_state][a1, a2] * sigma2_next[a2]\n",
    "\n",
    "    # Q gÃ¼ncelleme\n",
    "    Q1[state][action1, action2] = (1 - alpha) * Q1[state][action1, action2] + alpha * (r1 + gamma * expected_q1)\n",
    "    Q2[state][action1, action2] = (1 - alpha) * Q2[state][action1, action2] + alpha * (r2 + gamma * expected_q2)\n",
    "\n",
    "    # Durumu gÃ¼ncelle\n",
    "    state = next_state\n",
    "\n",
    "    # SkorlarÄ± gÃ¼ncelle\n",
    "    if r1 == 1:\n",
    "        score_1 += 1\n",
    "    elif r2 == 1:\n",
    "        score_2 += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "\n",
    "    # Her 200 turda durumu yazdÄ±r\n",
    "    if episode % 200 == 0:\n",
    "        print(f\"--- Tur {episode} ---\")\n",
    "        print(f\"AI_1 hamlesi: {move1} | AI_2 hamlesi: {move2}\")\n",
    "        print(f\"Skor: AI_1 = {score_1}, AI_2 = {score_2}, Beraberlik = {draws}\\n\")\n",
    "\n",
    "# Final skoru\n",
    "print(\"=== Oyun Sonu ===\")\n",
    "print(f\"AI_1 kazandÄ±: {score_1}\")\n",
    "print(f\"AI_2 kazandÄ±: {score_2}\")\n",
    "print(f\"Beraberlikler: {draws}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
