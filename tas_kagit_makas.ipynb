{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba3acbe",
   "metadata": {},
   "source": [
    "## Basit Taş Kağıt Makas Oyunu\n",
    "### Bu kodda bilgisayar tamamen rastgele oynuyor. Birkaç el oynayıp sonuçları gözlemleyelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf45f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Olası hamleler\n",
    "moves = ['taş', 'kağıt', 'makas']\n",
    "\n",
    "# Skorlar\n",
    "user_score = 0\n",
    "computer_score = 0\n",
    "\n",
    "# Kazanma kuralları\n",
    "def determine_winner(user_move, comp_move):\n",
    "    if user_move == comp_move:\n",
    "        return 'beraberlik'\n",
    "    elif (user_move == 'taş' and comp_move == 'makas') or \\\n",
    "         (user_move == 'kağıt' and comp_move == 'taş') or \\\n",
    "         (user_move == 'makas' and comp_move == 'kağıt'):\n",
    "        return 'kullanıcı'\n",
    "    else:\n",
    "        return 'bilgisayar'\n",
    "\n",
    "# Oyun döngüsü\n",
    "for round in range(1, 10):\n",
    "    print(f\"\\n--- {round}. Tur ---\")\n",
    "    user_move = input(\"Hamleni gir (taş/kağıt/makas): \").lower()\n",
    "    \n",
    "    if user_move not in moves:\n",
    "        print(\"Geçersiz hamle, tekrar deneyin.\")\n",
    "        continue\n",
    "    \n",
    "    comp_move = random.choice(moves)\n",
    "    print(f\"Bilgisayarın hamlesi: {comp_move}\")\n",
    "    \n",
    "    result = determine_winner(user_move, comp_move)\n",
    "    \n",
    "    if result == 'kullanıcı':\n",
    "        print(\"Bu turu sen kazandın!\")\n",
    "        user_score += 1\n",
    "    elif result == 'bilgisayar':\n",
    "        print(\"Bu turu bilgisayar kazandı!\")\n",
    "        computer_score += 1\n",
    "    else:\n",
    "        print(\"Beraberlik!\")\n",
    "\n",
    "print(f\"\\n🎯 Son Skor - Kullanıcı: {user_score} | Bilgisayar: {computer_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0c2f2",
   "metadata": {},
   "source": [
    "## Markov Modeli ile Taş-Kâğıt-Makas\n",
    "### Kullanıcının her hamlesinden sonra, bir sonraki hamlesi kaydedilir.\n",
    "\n",
    "### Geçmiş hamle istatistiklerine göre, kullanıcı bir hamleden sonra en çok ne yapmışsa, onu yapacağı varsayılır.\n",
    "\n",
    "### Bilgisayar da bu tahmine göre ona karşı gelen hamleyi seçer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786312c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "moves = ['taş', 'kağıt', 'makas']\n",
    "beats = {'taş': 'kağıt', 'kağıt': 'makas', 'makas': 'taş'}\n",
    "\n",
    "# 2. derece Markov: (prev2, prev1) -> sonraki hamle sayıları\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "user_history = []\n",
    "user_score = 0\n",
    "computer_score = 0\n",
    "\n",
    "def determine_winner(user_move, comp_move):\n",
    "    if user_move == comp_move:\n",
    "        return 'beraberlik'\n",
    "    elif (user_move == 'taş' and comp_move == 'makas') or \\\n",
    "         (user_move == 'kağıt' and comp_move == 'taş') or \\\n",
    "         (user_move == 'makas' and comp_move == 'kağıt'):\n",
    "        return 'kullanıcı'\n",
    "    else:\n",
    "        return 'bilgisayar'\n",
    "\n",
    "def predict_next_move(prev2, prev1):\n",
    "    key = (prev2, prev1)\n",
    "    if key not in transition_counts:\n",
    "        return random.choice(moves)\n",
    "    next_moves = transition_counts[key]\n",
    "    if not next_moves:\n",
    "        return random.choice(moves)\n",
    "    # En olası hareketi tahmin et\n",
    "    predicted = max(next_moves, key=next_moves.get)\n",
    "    return predicted\n",
    "\n",
    "for round in range(1, 11):\n",
    "    print(f\"\\n--- {round}. Tur ---\")\n",
    "    user_move = input(\"Hamleni gir (taş/kağıt/makas): \").lower()\n",
    "\n",
    "    if user_move not in moves:\n",
    "        print(\"Geçersiz hamle.\")\n",
    "        continue\n",
    "\n",
    "    # Tahmin ve seçim\n",
    "    if len(user_history) < 2:\n",
    "        comp_move = random.choice(moves)\n",
    "    else:\n",
    "        predicted_user_move = predict_next_move(user_history[-2], user_history[-1])\n",
    "        comp_move = beats[predicted_user_move]\n",
    "    \n",
    "    print(f\"Senin hamlen: {user_move} | Bilgisayarın hamlesi: {comp_move}\")\n",
    "    \n",
    "    result = determine_winner(user_move, comp_move)\n",
    "    if result == 'kullanıcı':\n",
    "        print(\"Bu turu sen kazandın!\")\n",
    "        user_score += 1\n",
    "    elif result == 'bilgisayar':\n",
    "        print(\"Bu turu bilgisayar kazandı!\")\n",
    "        computer_score += 1\n",
    "    else:\n",
    "        print(\"Beraberlik!\")\n",
    "\n",
    "    # Markov modelini güncelle\n",
    "    if len(user_history) >= 2:\n",
    "        key = (user_history[-2], user_history[-1])\n",
    "        transition_counts[key][user_move] += 1\n",
    "    user_history.append(user_move)\n",
    "\n",
    "print(f\"\\n📊 Son Skor - Kullanıcı: {user_score} | Bilgisayar: {computer_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4bf64",
   "metadata": {},
   "source": [
    "## Q-learning ile Taş-Kağıt-Makas Oyunu (Temel)\n",
    "### Durum (state): Kullanıcının son hamlesi (örneğin: taş, kağıt, makas)\n",
    "\n",
    "### Aksiyon (action): Bilgisayarın oynayacağı hamle (taş, kağıt, makas)\n",
    "\n",
    "### Ödül (reward): Kazanma = +1, Beraberlik = 0, Kaybetme = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e621fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "moves = ['taş', 'kağıt', 'makas']\n",
    "beats = {'taş': 'makas', 'kağıt': 'taş', 'makas': 'kağıt'}\n",
    "\n",
    "# Q-Tablosu: state (son kullanıcı hamlesi) -> action (bilgisayar hamlesi) değerleri\n",
    "Q = {move: {a: 0 for a in moves} for move in moves}\n",
    "\n",
    "alpha = 0.1    # öğrenme hızı\n",
    "gamma = 0.9    # gelecekteki ödüllerin indirgeme faktörü\n",
    "epsilon = 0.2  # keşif oranı\n",
    "\n",
    "def determine_winner(user_move, comp_move):\n",
    "    if user_move == comp_move:\n",
    "        return 0  # beraberlik\n",
    "    elif beats[comp_move] == user_move:\n",
    "        return 1  # bilgisayar kazandı\n",
    "    else:\n",
    "        return -1 # kullanıcı kazandı\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(moves)  # keşif\n",
    "    else:\n",
    "        # En yüksek Q değerine sahip hareketi seç\n",
    "        max_q = max(Q[state].values())\n",
    "        actions_with_max_q = [a for a, q in Q[state].items() if q == max_q]\n",
    "        return random.choice(actions_with_max_q)\n",
    "\n",
    "user_history = []\n",
    "user_score = 0\n",
    "computer_score = 0\n",
    "\n",
    "print(\"Taş-Kağıt-Makas Q-learning oyunu başlıyor!\")\n",
    "\n",
    "for round in range(1, 21):\n",
    "    print(f\"\\n--- {round}. Tur ---\")\n",
    "    user_move = input(\"Hamleni gir (taş/kağıt/makas): \").lower()\n",
    "    if user_move not in moves:\n",
    "        print(\"Geçersiz hamle.\")\n",
    "        continue\n",
    "    \n",
    "    # Son kullanıcı hamlesine göre durum belirle\n",
    "    if not user_history:\n",
    "        state = random.choice(moves)  # Başlangıç için rastgele durum\n",
    "    else:\n",
    "        state = user_history[-1]\n",
    "    \n",
    "    comp_move = choose_action(state)\n",
    "    print(f\"Senin hamlen: {user_move} | Bilgisayarın hamlesi: {comp_move}\")\n",
    "    \n",
    "    reward = determine_winner(user_move, comp_move)\n",
    "    if reward == 1:\n",
    "        print(\"Bilgisayar kazandı!\")\n",
    "        computer_score += 1\n",
    "    elif reward == -1:\n",
    "        print(\"Sen kazandın!\")\n",
    "        user_score += 1\n",
    "    else:\n",
    "        print(\"Beraberlik!\")\n",
    "\n",
    "    # Q-Tablosunu güncelle\n",
    "    if user_history:\n",
    "        old_state = user_history[-1]\n",
    "        old_action = comp_move\n",
    "        # Bir sonraki durum\n",
    "        next_state = user_move\n",
    "        max_future_q = max(Q[next_state].values())\n",
    "        Q[old_state][old_action] = Q[old_state][old_action] + alpha * (reward + gamma * max_future_q - Q[old_state][old_action])\n",
    "\n",
    "    user_history.append(user_move)\n",
    "\n",
    "print(f\"\\n📊 Son Skor - Kullanıcı: {user_score} | Bilgisayar: {computer_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896cc19",
   "metadata": {},
   "source": [
    "## İki AI Ajanıyla Q-learning Taş-Kağıt-Makas Oyunu\n",
    "### Durum (state): Rakibin önceki hamlesi (taş, kağıt veya makas)\n",
    "### İlk hamle için özel durum: \"Başlangıç\" (rakibin hamlesi yok)\n",
    "### Aksiyon (action): Oyuncunun oynayabileceği hamleler: taş, kağıt, makas\n",
    "### Ödül (reward): Kazanma = +1 Beraberlik = 0 Kaybetme = -1\n",
    "\n",
    "### Her bir AI, rakibin son hamlesini gözlemleyerek hangi hamleyi yaparsa kazanma şansının yüksek olduğunu Q-learning ile öğrenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee9f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tur 100 ---\n",
      "AI_1 Skor: 24, AI_2 Skor: 30, Beraberlik: 46\n",
      "--- Tur 200 ---\n",
      "AI_1 Skor: 67, AI_2 Skor: 60, Beraberlik: 73\n",
      "--- Tur 300 ---\n",
      "AI_1 Skor: 94, AI_2 Skor: 82, Beraberlik: 124\n",
      "--- Tur 400 ---\n",
      "AI_1 Skor: 135, AI_2 Skor: 126, Beraberlik: 139\n",
      "--- Tur 500 ---\n",
      "AI_1 Skor: 167, AI_2 Skor: 172, Beraberlik: 161\n",
      "\n",
      "📊 Oyun Sonucu:\n",
      "AI_1 kazandığı tur sayısı: 167\n",
      "AI_2 kazandığı tur sayısı: 172\n",
      "Beraberlik sayısı: 161\n",
      "Kazanan: AI_2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "moves = ['taş', 'kağıt', 'makas']\n",
    "beats = {'taş': 'makas', 'kağıt': 'taş', 'makas': 'kağıt'}\n",
    "\n",
    "# Q-learning parametreleri\n",
    "alpha = 0.1    # öğrenme hızı\n",
    "gamma = 0.9    # indirim faktörü\n",
    "epsilon = 0.1  # keşif oranı\n",
    "episodes = 500\n",
    "\n",
    "# Q tabloları (state=son hamle, action=hamle)\n",
    "Q1 = {move: np.zeros(len(moves)) for move in moves + ['Başlangıç']}\n",
    "Q2 = {move: np.zeros(len(moves)) for move in moves + ['Başlangıç']}\n",
    "\n",
    "def choose_action(Q, state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(range(len(moves)))\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def reward(p1_move, p2_move):\n",
    "    if p1_move == p2_move:\n",
    "        return 0\n",
    "    elif beats[p1_move] == p2_move:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Oyun sonucu skorları\n",
    "ai1_score = 0\n",
    "ai2_score = 0\n",
    "draw_count = 0\n",
    "\n",
    "state1 = 'Başlangıç'\n",
    "state2 = 'Başlangıç'\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    action1 = choose_action(Q1, state1)\n",
    "    action2 = choose_action(Q2, state2)\n",
    "\n",
    "    move1 = moves[action1]\n",
    "    move2 = moves[action2]\n",
    "\n",
    "    r1 = reward(move1, move2)\n",
    "    r2 = reward(move2, move1)  # tersi\n",
    "\n",
    "    # Q-table güncelleme\n",
    "    next_state1 = move2\n",
    "    next_state2 = move1\n",
    "\n",
    "    Q1[state1][action1] = Q1[state1][action1] + alpha * (r1 + gamma * max(Q1[next_state1]) - Q1[state1][action1])\n",
    "    Q2[state2][action2] = Q2[state2][action2] + alpha * (r2 + gamma * max(Q2[next_state2]) - Q2[state2][action2])\n",
    "\n",
    "    state1 = next_state1\n",
    "    state2 = next_state2\n",
    "\n",
    "    # Skor güncelleme\n",
    "    if r1 == 1:\n",
    "        ai1_score += 1\n",
    "    elif r1 == -1:\n",
    "        ai2_score += 1\n",
    "    else:\n",
    "        draw_count += 1\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"--- Tur {episode} ---\")\n",
    "        print(f\"AI_1 Skor: {ai1_score}, AI_2 Skor: {ai2_score}, Beraberlik: {draw_count}\")\n",
    "\n",
    "print(\"\\n📊 Oyun Sonucu:\")\n",
    "print(f\"AI_1 kazandığı tur sayısı: {ai1_score}\")\n",
    "print(f\"AI_2 kazandığı tur sayısı: {ai2_score}\")\n",
    "print(f\"Beraberlik sayısı: {draw_count}\")\n",
    "\n",
    "if ai1_score > ai2_score:\n",
    "    print(\"Kazanan: AI_1\")\n",
    "elif ai2_score > ai1_score:\n",
    "    print(\"Kazanan: AI_2\")\n",
    "else:\n",
    "    print(\"Oyun berabere bitti!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2cea56",
   "metadata": {},
   "source": [
    "## Nash Q-learning ile İki AI Ajanının Taş-Kağıt-Makas Oyunu  \n",
    "### Durum (state): Rakibin önceki hamlesi (taş, kağıt veya makas) veya \"Başlangıç\"  \n",
    "### Aksiyon (action): Oyuncunun oynayabileceği hamleler: taş, kağıt, makas  \n",
    "### Ödül (reward): Kazanma = +1, Beraberlik = 0, Kaybetme = -1  \n",
    "\n",
    "### Özet:  \n",
    "- Her turda iki AI ajanı, mevcut durumdaki Q-matrislerine göre oyun matrisini oluşturur.  \n",
    "- Nashpy kütüphanesi kullanılarak bu matrisler için Nash dengesi (karşılıklı en iyi strateji karışımı) hesaplanır.  \n",
    "- Ajanlar, Nash dengesinden elde edilen karma stratejilere göre hareket seçer.  \n",
    "- Q tabloları, ödül ve gelecekteki Nash dengesine göre beklenen getirilerle güncellenir.  \n",
    "- Böylece ajanlar sadece kendi ödüllerini değil, karşı tarafın stratejik davranışlarını da hesaba katarak öğrenir.  \n",
    "- Bu yöntem, klasik Q-learning’den farklı olarak çok oyunculu oyunlarda denge stratejilerini öğrenmeyi hedefler.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8022fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nashpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5d0fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tur 200 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 200\n",
      "\n",
      "--- Tur 400 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 400\n",
      "\n",
      "--- Tur 600 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 600\n",
      "\n",
      "--- Tur 800 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 800\n",
      "\n",
      "--- Tur 1000 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1000\n",
      "\n",
      "--- Tur 1200 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1200\n",
      "\n",
      "--- Tur 1400 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1400\n",
      "\n",
      "--- Tur 1600 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1600\n",
      "\n",
      "--- Tur 1800 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 1800\n",
      "\n",
      "--- Tur 2000 ---\n",
      "AI_1 hamlesi: taş | AI_2 hamlesi: taş\n",
      "Skor: AI_1 = 0, AI_2 = 0, Beraberlik = 2000\n",
      "\n",
      "=== Oyun Sonu ===\n",
      "AI_1 kazandı: 0\n",
      "AI_2 kazandı: 0\n",
      "Beraberlikler: 2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nashpy as nash\n",
    "\n",
    "# Taş-kağıt-makas tanımı\n",
    "moves = ['taş', 'kağıt', 'makas']\n",
    "num_moves = len(moves)\n",
    "\n",
    "# Parametreler\n",
    "alpha = 0.1       # öğrenme hızı\n",
    "gamma = 0.9       # indirim faktörü\n",
    "episodes = 2000   # toplam oyun sayısı\n",
    "\n",
    "# Q tabloları: durum 'Başlangıç' için 3x3 matrisler\n",
    "Q1 = {'Başlangıç': np.zeros((num_moves, num_moves))}\n",
    "Q2 = {'Başlangıç': np.zeros((num_moves, num_moves))}\n",
    "\n",
    "def reward(p1_move, p2_move):\n",
    "    if p1_move == p2_move:\n",
    "        return 0, 0\n",
    "    elif (p1_move == 'taş' and p2_move == 'makas') or \\\n",
    "         (p1_move == 'kağıt' and p2_move == 'taş') or \\\n",
    "         (p1_move == 'makas' and p2_move == 'kağıt'):\n",
    "        return 1, -1\n",
    "    else:\n",
    "        return -1, 1\n",
    "\n",
    "def get_nash_strategy(Q1_mat, Q2_mat):\n",
    "    \"\"\"\n",
    "    Verilen iki Q matrisi için Nash dengesini hesaplar.\n",
    "    \"\"\"\n",
    "    game = nash.Game(Q1_mat, Q2_mat)\n",
    "    equilibria = list(game.support_enumeration())\n",
    "    if equilibria:\n",
    "        sigma1, sigma2 = equilibria[0]\n",
    "    else:\n",
    "        # Nash bulunamazsa eşit olasılık dağılımı\n",
    "        sigma1 = np.ones(num_moves) / num_moves\n",
    "        sigma2 = np.ones(num_moves) / num_moves\n",
    "    return sigma1, sigma2\n",
    "\n",
    "# Başlangıç durumu\n",
    "state = 'Başlangıç'\n",
    "\n",
    "# Kazanma sayacı\n",
    "score_1 = 0\n",
    "score_2 = 0\n",
    "draws = 0\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    # Durum için Q tablosu yoksa oluştur\n",
    "    if state not in Q1:\n",
    "        Q1[state] = np.zeros((num_moves, num_moves))\n",
    "        Q2[state] = np.zeros((num_moves, num_moves))\n",
    "\n",
    "    Q1_state = Q1[state]\n",
    "    Q2_state = Q2[state]\n",
    "\n",
    "    # Nash stratejilerini bul\n",
    "    sigma1, sigma2 = get_nash_strategy(Q1_state, Q2_state)\n",
    "\n",
    "    # Stratejilere göre hamle seç\n",
    "    action1 = np.random.choice(num_moves, p=sigma1)\n",
    "    action2 = np.random.choice(num_moves, p=sigma2)\n",
    "\n",
    "    move1 = moves[action1]\n",
    "    move2 = moves[action2]\n",
    "\n",
    "    # Ödüller\n",
    "    r1, r2 = reward(move1, move2)\n",
    "\n",
    "    # Gelecek durum (örnek basit: rakibin son hamlesi)\n",
    "    next_state = move2\n",
    "    if next_state not in Q1:\n",
    "        Q1[next_state] = np.zeros((num_moves, num_moves))\n",
    "        Q2[next_state] = np.zeros((num_moves, num_moves))\n",
    "\n",
    "    # Gelecek durumun Nash stratejileri\n",
    "    sigma1_next, sigma2_next = get_nash_strategy(Q1[next_state], Q2[next_state])\n",
    "\n",
    "    # Gelecek durumun beklenen ödülleri\n",
    "    expected_q1 = 0\n",
    "    expected_q2 = 0\n",
    "    for a1 in range(num_moves):\n",
    "        for a2 in range(num_moves):\n",
    "            expected_q1 += sigma1_next[a1] * Q1[next_state][a1, a2] * sigma2_next[a2]\n",
    "            expected_q2 += sigma1_next[a1] * Q2[next_state][a1, a2] * sigma2_next[a2]\n",
    "\n",
    "    # Q güncelleme\n",
    "    Q1[state][action1, action2] = (1 - alpha) * Q1[state][action1, action2] + alpha * (r1 + gamma * expected_q1)\n",
    "    Q2[state][action1, action2] = (1 - alpha) * Q2[state][action1, action2] + alpha * (r2 + gamma * expected_q2)\n",
    "\n",
    "    # Durumu güncelle\n",
    "    state = next_state\n",
    "\n",
    "    # Skorları güncelle\n",
    "    if r1 == 1:\n",
    "        score_1 += 1\n",
    "    elif r2 == 1:\n",
    "        score_2 += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "\n",
    "    # Her 200 turda durumu yazdır\n",
    "    if episode % 200 == 0:\n",
    "        print(f\"--- Tur {episode} ---\")\n",
    "        print(f\"AI_1 hamlesi: {move1} | AI_2 hamlesi: {move2}\")\n",
    "        print(f\"Skor: AI_1 = {score_1}, AI_2 = {score_2}, Beraberlik = {draws}\\n\")\n",
    "\n",
    "# Final skoru\n",
    "print(\"=== Oyun Sonu ===\")\n",
    "print(f\"AI_1 kazandı: {score_1}\")\n",
    "print(f\"AI_2 kazandı: {score_2}\")\n",
    "print(f\"Beraberlikler: {draws}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
